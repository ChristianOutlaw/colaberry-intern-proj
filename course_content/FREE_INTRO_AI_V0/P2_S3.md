# Preparing Data for AI

## What you'll learn
- Understand why clean, well-structured data is required before any AI training begins
- Walk through the key steps in data preparation
- See why human judgment is essential throughout the pipeline

## Key ideas
- Data preparation happens before model building — and it's usually the hardest part
- Garbage in, garbage out: data quality directly determines prediction quality
- Humans define what matters; algorithms find patterns within those boundaries

## Lesson

### Data Preparation Comes First

Before a single AI model is trained, data goes through a preparation phase. This is often the most time-consuming part of any AI project — and the most consequential. Skipping or rushing it is the most common reason projects fail.

**Common preparation steps:**

**1. Data cleaning**
Remove or correct errors: duplicate records, impossible values, inconsistent formatting.
- Example: If one column stores age as "29", "29 years", and "twenty-nine", the algorithm can't compare those values reliably.

**2. Handling missing values**
Decide what to do when records are incomplete: remove the rows, fill in with averages, or flag the gap explicitly and let the model account for it.

**3. Normalization and scaling**
Put different measurements on the same scale so one feature doesn't dominate unfairly.
- Example: Age (18–80) and annual income (20,000–200,000) need to be scaled before being compared in the same model.

**4. Feature selection**
Choose which columns (features) are actually relevant to the prediction goal. More isn't always better — irrelevant features add noise and can reduce accuracy.

### Humans Set the Boundaries

The algorithm doesn't decide which features matter. A human analyst looks at the data and asks:
- What am I actually trying to predict?
- Which variables are likely to be informative?
- Which variables might accidentally encode bias?

A hiring model that includes zip code as a feature might inadvertently learn a proxy for race or socioeconomic background. Removing it — or explicitly correcting for it — is a human decision that must happen before training starts.

> **The quality of an AI system is largely determined before the algorithm ever runs — in how the data was collected, cleaned, and prepared.**

### AI Builds on Analytics

Once data is clean and structured, AI can extend what analytics started. Analytics describes the past. AI uses those historical patterns to make predictions about the future.

The full flow: **Collect → Clean → Analyze → Visualize → Train AI → Predict**

Each stage depends on the one before it. AI at the end of a bad pipeline produces bad predictions, at scale.

## Common mistakes

**"AI can automatically fix messy processes or bad underlying data."**

It can't. AI scales what already exists in the data — it doesn't detect or correct unclear goals or broken workflows. If a business defines "customer churn" inconsistently across teams, the model will learn those inconsistencies and reproduce them in its predictions. Fix the process first, then apply AI to it.

## Quick check

**Q1.** How does AI build on analytics?

- A) AI predicts what might happen next using patterns learned from past analytic data ✓
- B) AI ignores analytics and works from raw unprocessed data
- C) AI replaces the need for data analysis entirely
- D) AI works best with randomly sampled, uncleaned data

**Q2.** Why is feature selection an important step before training a model?

- A) More features always improve model accuracy
- B) Irrelevant or biased features add noise and can reduce prediction quality ✓
- C) Feature selection eliminates the need for data cleaning
- D) Only the algorithm — not humans — should choose which features to include

## Reflection

**How confident are you right now in your understanding of how AI learns from data?**

- Low — I'm still working through some of the concepts
- Medium — I understand the basics but have some open questions
- High — I could explain this pipeline to someone else
