# P3_S2 — Evaluating Results

## Summary

Explains accuracy, precision, and recall in plain language; shows how to judge
whether a model is trustworthy.

## Learning Objectives

- Define accuracy, precision, and recall without using formulas.
- Explain a scenario where high accuracy can still mean a bad model.
- Decide which metric matters most for a given business problem.

## Outline

- Why evaluation comes after training, not before
- Accuracy: right answers / all answers — and when it misleads
- The imbalanced class problem: 99 % accuracy on a rare disease detector
- Precision: when false positives are costly (spam filter, fraud alert)
- Recall: when false negatives are costly (cancer screening, safety alerts)
- Overfitting in plain language: memorized vs. generalized
- The test set: why we hold data back
- Rule of thumb: always ask "what does a wrong answer cost?"

## Hands-on (optional)

*Placeholder: Learner reads two model result cards and selects which model
they would deploy for a given scenario, explaining their reasoning in one sentence.*

## Completion Criteria

Learner completes a 3-question check. Score ≥ 2/3 triggers a `ProgressEvent`
with `section_id = "P3_S2"`.

## Assets / Links

- *(placeholder: model result card examples)*
- *(placeholder: precision vs. recall visual)*
